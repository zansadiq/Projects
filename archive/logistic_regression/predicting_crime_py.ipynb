{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Crime:\n",
    "\n",
    "## Introduction-\n",
    "\n",
    "In this post, we will be trying to predict the category of a crime based on various statistics provided by the Police Department. \n",
    "\n",
    "This task provides excellent practice for a number of technical areas of data science.  \n",
    "\n",
    "First of all, it provides a large set of data for us to work with. The file contains 6,000,000 observations. Next, it provides us with a classification task upon which we can test a number of different algorithms. Also, it provides location information of crimes so we can experiment with some of the R language's plotting functions as well.  \n",
    "\n",
    "For starters, the data can be downloaded [here](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2).  \n",
    "\n",
    "## Data Transformation-\n",
    "\n",
    "The first task of any data science project is to become familiar with the data through exploration. We must also manipulate and transform it into a useful format that we can then develop insight from.  \n",
    "\n",
    "In order to process our data before we can feed it into our machine learning algorithms, we must clean it with the following steps:\n",
    "\n",
    "1. Remove spaces from the headers\n",
    "2. Convert all headers to lower-case\n",
    "3. Parse and format the dates\n",
    "4. Merge specific columns\n",
    "\n",
    "Lets dive right in, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/files/chicago_crime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1df703f01861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/path/to/files/chicago_crime\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/files/chicago_crime'"
     ]
    }
   ],
   "source": [
    "# Set directory to appropriate folder\n",
    "path = \"/path/to/files/chicago_crime\"\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"chicago_crime.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is somewhat controversial. The general rule of thumb within the data science community is that if records with missing values account for less than 5% of the total- they can be deleted outright, otherwise the missing values should be imputed.\n",
    "\n",
    "For the sake of convenience, we are going to simply delete all rows with missing data rather than imputing their values. This is mainly just because the dataset is already so large that we will still have plenty of records for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing information\n",
    "data1 = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the headers\n",
    "\n",
    "# Remove spaces\n",
    "data1.columns = data1.columns.str.replace('\\s+', '_')\n",
    "\n",
    "# Remove upper case\n",
    "data1.columns = map(str.lower, data1.columns)\n",
    "\n",
    "# Fix date and time\n",
    "data1['date'] = pd.to_datetime(data1['date'], format = '%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# Drop unimportant columns\n",
    "data2 = data1.drop(columns = ['date', 'id', 'case_number', 'block', 'updated_on', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables\n",
    "data2['iucr'] = pd.factorize(data2.iucr)[0]\n",
    "data2['primary_type'] = pd.factorize(data2.primary_type)[0]\n",
    "data2['arrest'] = pd.factorize(data2.arrest)[0]\n",
    "data2['description'] = pd.factorize(data2.description)[0]\n",
    "data2['location_description'] = pd.factorize(data2.location_description)[0]\n",
    "data2['fbi_code'] = pd.factorize(data2.fbi_code)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning-\n",
    "\n",
    "Now that we have cleaned up our data and added some formatting to it, we are ready to get started with actually implementing some models and making predictions. \n",
    "\n",
    "For the sake of simplicity, we will first work on trying to predict whether or not an arrest was made in each situation.\n",
    "\n",
    "### Predicting Arrest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Since the arrest category is a binary variable, it is a prime candidate for logistic regression.\n",
    "\n",
    "However, before we begin- lets take a quick look at the actual breakdown of the arrest category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table\n",
    "data2['arrest'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual\n",
    "sns.countplot(x = 'arrest', data = data2)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this information, we can already see that the majority of incidents actually do not end up in an arrest. Overall, however, the distribution is still such that we shouldnt have to worry about a \"class-imbalance\" issue arising. \n",
    "\n",
    "One of the assumptions made by the logistic regression model is that the independent variables are uncorrelated to one another. Lets go ahead and check to see if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "sns.heatmap(data2.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heatmap shows that all of the variables except those relating to geography are relatively un-correlated. For this example, we will go ahead and accept the correlation- otherwise we would have to remove some variables using a feature selection method. \n",
    "\n",
    "The next step is to convert the categorical variables so that they can be interpreted by the model. Normally, we would do this by creating a dummy variable for each individual factor level. The problem with this approach is that it would lead to too many new columns and hence the \"curse of dimensionality\". \n",
    "\n",
    "Instead, we will try a different approach and encode the factors as numeric variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8f57ec0740a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check column types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data2' is not defined"
     ]
    }
   ],
   "source": [
    "# Check column types\n",
    "data2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the independent and dependent variables\n",
    "labels = data3['arrest']\n",
    "\n",
    "features = data3.loc[:, data3.columns != 'arrest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = split(features, labels, test_size = .25, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected all of our variables and appropriately encoded them and split them into training and testing sets; we are ready to actually implement the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "test_pred = logreg.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the performance metrics\n",
    "logreg.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion- \n",
    "\n",
    "So, our final accuracy on the test set came out to 72%. This isnt bad considering the obstacles we faced, as well as the simplified procedure. \n",
    "\n",
    "In order to enhance the accuracy of this model, there are several additional steps we could take such as:\n",
    "\n",
    "* Feature selection\n",
    "* Cross-validation\n",
    "* Ensemble modeling\n",
    "* and more...\n",
    "\n",
    "These steps we will save for a later date."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
